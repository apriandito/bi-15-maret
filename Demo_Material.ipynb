{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apriandito/bi-15-maret/blob/main/Demo_Material.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZLt1HEJvpQA"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "eZTNUy-ImHeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M0gjIG1NjtI"
      },
      "source": [
        "## **Collecting Twitter Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL7-ah8IC7Bk"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import tweepy \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5fH7hKtDC8y"
      },
      "outputs": [],
      "source": [
        "# Authentication\n",
        "consumer_key = 'fbqT0mBw9ehG9IhmT2XTSb0Ti'\n",
        "consumer_secret = 'MJaEGWTR396VqFWZQDmnybqGwbvydCTFFJ9KFLtsMH7vMgfmkr'\n",
        "access_token = '1323866871288877057-mhauIZAemtLfS4gPxWlyPe6EyGuszv'\n",
        "access_token_secret = 'fDGMlo6xREFKr4gs3qD6GgoetdFeiFz1DdwuKPJHSsaDL'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m_0yqa4DELA"
      },
      "outputs": [],
      "source": [
        "# Set up your authentication to Twitter API\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Create an API object\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BpfHiMoDHrd"
      },
      "outputs": [],
      "source": [
        "# Collect tweets\n",
        "tweets = []\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q='Bima', lang = \"id\").items(100):\n",
        "    tweets.append(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17bYi6ZUIG44"
      },
      "outputs": [],
      "source": [
        "# Create Pandas dataframe\n",
        "df_tweet = pd.DataFrame({'time' : [tweet.created_at for tweet in tweets],\n",
        "                   'screen_name' : [tweet.user.screen_name for tweet in tweets],\n",
        "                   'text' : [tweet.text for tweet in tweets],\n",
        "                   'reply_to_screen_name' : [tweet.in_reply_to_screen_name for tweet in tweets],\n",
        "                   'location' : [tweet.user.location for tweet in tweets]})\n",
        "\n",
        "# Show tweets data\n",
        "df_tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhdGH3_4LAWD"
      },
      "outputs": [],
      "source": [
        "# Save tweets data\n",
        "df_tweet.to_csv('tweets.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDwwFvcMMHEY"
      },
      "source": [
        "## **Import Data (Tweet Data Collected using Phantombuster**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9BDlruBMXuD"
      },
      "outputs": [],
      "source": [
        "# Import Data\n",
        "df_tweet = pd.read_csv(\"https://raw.githubusercontent.com/apriandito/bi-15-maret/main/data/result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxAr2CKZMnsN"
      },
      "outputs": [],
      "source": [
        "# Show Data\n",
        "df_tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sujPPFBpNnJi"
      },
      "source": [
        "## **Network Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73wCdV1eNr0R"
      },
      "source": [
        "### **Create Edgelist**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJbNZSfbTJ8t"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import re\n",
        "\n",
        "# Create empty edgelist\n",
        "edge_list = []\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df_tweet.iterrows():\n",
        "    account_name = row['handle']\n",
        "    tweet = row['text']\n",
        "    \n",
        "    # Use regex to find mentions in the tweet\n",
        "    mentions = re.findall(r'@(\\w+)', tweet)\n",
        "    \n",
        "    # Create edges between the account_name and each mentioned account\n",
        "    edges = [(account_name, mention) for mention in mentions]\n",
        "    \n",
        "    # Extend the edge list with the newly created edges\n",
        "    edge_list.extend(edges)\n",
        "\n",
        "# Create a new DataFrame from the edge list\n",
        "edgelist = pd.DataFrame(edge_list, columns=['source', 'target'])\n",
        "edgelist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlmFXsgZUJ27"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import networkx as nx\n",
        "\n",
        "# Create a graph from the edgelist\n",
        "G = nx.from_pandas_edgelist(edgelist, source='source', target='target')\n",
        "\n",
        "# Visualize the graph\n",
        "nx.draw_networkx(G, pos = nx.kamada_kawai_layout(G), font_size = 7, with_labels= True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLiCOi7ZQ3vy"
      },
      "source": [
        "### **Network Properties Measurement**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGpnu1s6WlUf"
      },
      "outputs": [],
      "source": [
        "# Calculate number of nodes\n",
        "num_nodes = G.number_of_nodes()\n",
        "\n",
        "# Calculate number of edges\n",
        "num_edges = G.number_of_edges()\n",
        "\n",
        "# Calculate density\n",
        "density = nx.density(G)\n",
        "\n",
        "# Create a dataframe based on the results\n",
        "network_properties = pd.DataFrame(data= {'num_nodes':[num_nodes], \n",
        "                                 'num_edges':[num_edges],\n",
        "                                 'density':[density]})\n",
        "# Show network properties\n",
        "network_properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SpQLIllIdB0"
      },
      "outputs": [],
      "source": [
        "# Calculate degree centrality\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "\n",
        "# Calculate betweeness centrality\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "# Calculate closeness centrality\n",
        "closeness_centrality = nx.closeness_centrality(G)\n",
        "\n",
        "# Calculate closeness centrality\n",
        "eigenvector_centrality = nx.closeness_centrality(G)\n",
        "\n",
        "# Create a dataframe based on the results\n",
        "centralities = pd.DataFrame([degree_centrality, betweenness_centrality,\n",
        "                             closeness_centrality, eigenvector_centrality]).T\n",
        "centralities.columns = ['degree_centrality', 'betweenness_centrality',\n",
        "                        'closeness_centrality', 'eigenvector_centrality']\n",
        "\n",
        "# Show centralities\n",
        "centralities      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeaCYbp51LIq"
      },
      "source": [
        "### **Modularity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLFuD3tU1KX1"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "from community import community_louvain\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ityaRzHc1ZyB"
      },
      "outputs": [],
      "source": [
        "# Calculate the modularity\n",
        "partition = community_louvain.best_partition(G)\n",
        "values = [partition.get(node) for node in G.nodes()]\n",
        "partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcnNFq091ZaR"
      },
      "outputs": [],
      "source": [
        "# Visualize the graph\n",
        "nx.draw_networkx(G, pos = nx.kamada_kawai_layout(G), cmap = plt.get_cmap('jet'), node_color = values, font_size = 7, with_labels= True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-6z6yVIPCev"
      },
      "source": [
        "## **Collecting News Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx2uB64jRe8G"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFNc8to4RgX9"
      },
      "outputs": [],
      "source": [
        "# Specify the News API key and base url\n",
        "base_url = \"http://newsapi.org/v2/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI1WWNQFOjOp"
      },
      "outputs": [],
      "source": [
        "# Specify the parameters of the data you want to collect\n",
        "params = {\n",
        "    'q': 'artificial intelligence', #keywords to search for in the news\n",
        "    'sortBy': 'publishedAt', #sort the news by publication date\n",
        "    'apiKey': 'f2b2e3f10bee4668ba146f9bdff912fd' #specify the News API key\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OohMN8puRmYi"
      },
      "outputs": [],
      "source": [
        "# Call the API and convert the response to json format\n",
        "response = requests.get(base_url + 'everything', params=params)\n",
        "data = response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-vUjvLxRrYW"
      },
      "outputs": [],
      "source": [
        "#create an empty list to store the data\n",
        "news_data = []\n",
        "\n",
        "# Loop through each article and extract relevant fields to the list\n",
        "for article in data['articles']:\n",
        "    news_data.append({\n",
        "        'title': article['title'],\n",
        "        'description': article['description'],\n",
        "        'content': article['content'],\n",
        "        'url': article['url'],\n",
        "        'publishedAt': article['publishedAt'],\n",
        "        'source': article['source']['name']\n",
        "    })\n",
        "\n",
        "# Convert the list of dictionaries to a dataframe\n",
        "df_news = pd.DataFrame(news_data)\n",
        "\n",
        "# Show news data\n",
        "df_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXAi10PXRvKN"
      },
      "outputs": [],
      "source": [
        "# Save news data\n",
        "df_news.to_csv(\"news.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39kZ734CUScF"
      },
      "source": [
        "## **Sentiment Analysis: VADER (English)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBML3HF9OViA"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KClHOPIJSXTh"
      },
      "outputs": [],
      "source": [
        "# Download the corpus\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phg-V_wOO80r"
      },
      "outputs": [],
      "source": [
        "# Create a vader sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Create a list of sentences\n",
        "sentences = df_news['title']\n",
        "\n",
        "# Calculate the compound sentiment score of each sentence\n",
        "scores = [sia.polarity_scores(sentence)['compound'] for sentence in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkkHJxXEShX5"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualize the compound sentiment using seaborn\n",
        "sns.distplot(x=scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3jScZiTEjf"
      },
      "source": [
        "## **Topic Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mn7AB6w5z1r"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import gensim\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim import corpora\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g4Qia2Q98uS"
      },
      "outputs": [],
      "source": [
        "# Select News Title\n",
        "news_title = df_news['title']\n",
        "\n",
        "# Tokenize\n",
        "tokenized_text = [d.lower().split() for d in news_title]\n",
        "\n",
        "# Remove punctuation\n",
        "punctuation = string.punctuation\n",
        "tokenized_text = [[word for word in doc if word not in punctuation] for doc in tokenized_text]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenized_text = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokenized_text]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_text = [[word for word in doc if word not in stop_words] for doc in tokenized_text]\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(tokenized_text)\n",
        "\n",
        "# Create corpus\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53FC4XGg9fXE"
      },
      "outputs": [],
      "source": [
        "# Train LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=3,\n",
        "                                           passes = 100, \n",
        "                                           per_word_topics=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVqwRX_s9-iq"
      },
      "outputs": [],
      "source": [
        "# Enable Notebook \n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "# Visualize\n",
        "pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICZUUznNTKnD"
      },
      "source": [
        "## **Text Network Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX69y5wsD8Ye"
      },
      "outputs": [],
      "source": [
        "def remove_non_text(value):\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQqVbbw-eNKu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create dataframe\n",
        "df_tna = pd.DataFrame(df_news['title'].str.lower().apply(remove_non_text))\n",
        "df_tna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWRGDkYcAPrE"
      },
      "outputs": [],
      "source": [
        "# Create empty graph\n",
        "G = nx.Graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ig0VgQpAc1A"
      },
      "outputs": [],
      "source": [
        "# For each row in dataframe\n",
        "for index, row in df_tna.iterrows():\n",
        "    \n",
        "    # Split tittle into words\n",
        "    words = row['title'].split(' ')\n",
        "    \n",
        "    # Add relationships between words\n",
        "    for w1 in words:\n",
        "        for w2 in words:\n",
        "            if w1 != w2:\n",
        "                G.add_edge(w1,w2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJL_xlQbAjDc"
      },
      "outputs": [],
      "source": [
        "# Visualize the graph\n",
        "nx.draw(G, font_size = 7, with_labels = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDUZzG0xQKaI"
      },
      "source": [
        "## **Sentiment Analysis: IndoBERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw1mxj2JQWzq"
      },
      "outputs": [],
      "source": [
        "# Install Huggingface Transformers\n",
        "! pip install  huggingface transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7Jda8DaSt0K"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries \n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBQjyYIwQLTl"
      },
      "outputs": [],
      "source": [
        "# Download Pretrained Model\n",
        "pretrained= \"mdhugol/indonesia-bert-sentiment-classification\"\n",
        "\n",
        "# Set Model and Tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(pretrained)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained)\n",
        "\n",
        "# Create sentiment classifier using huggingface pipeline\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCq8skXQQxli"
      },
      "outputs": [],
      "source": [
        "# Get sentiment label for each row in dataframe \n",
        "df_tweet['sentiment'] = df_tweet['text'].apply(lambda x: sentiment_analysis(x)[0]['label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnhAo-kYXVD2"
      },
      "outputs": [],
      "source": [
        "# Set Label\n",
        "label_index = {'LABEL_0': 'positive', 'LABEL_1': 'neutral', 'LABEL_2': 'negative'}\n",
        "\n",
        "# Replace the values in the sentiment column\n",
        "df_tweet['sentiment'] = df_tweet['sentiment'].replace(label_index)\n",
        "\n",
        "# Show Tweet with sentiment\n",
        "df_tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXtoH2KTVM0H"
      },
      "outputs": [],
      "source": [
        "# Visualise the sentiment distribution\n",
        "sns.countplot(x ='sentiment', data = df_tweet)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMujhv+vkRHnNJ3ID9/N12",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}